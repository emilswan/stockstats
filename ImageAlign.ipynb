{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPS1WWpg6sGRKZwp1OEppQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilswan/stockstats/blob/master/ImageAlign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "IyoQheMqWc0I",
        "outputId": "e15ec924-172f-407f-9022-33bea10a740b"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import imutils\n",
        "import cv2\n",
        "def align_images(image, template, maxFeatures=500, keepPercent=0.2,\n",
        "\tdebug=False):\n",
        "\t# convert both the input image and template to grayscale\n",
        "\timageGray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\ttemplateGray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
        " \n",
        " \t# use ORB to detect keypoints and extract (binary) local\n",
        "\t# invariant features\n",
        "\torb = cv2.ORB_create(maxFeatures)\n",
        "\t(kpsA, descsA) = orb.detectAndCompute(imageGray, None)\n",
        "\t(kpsB, descsB) = orb.detectAndCompute(templateGray, None)\n",
        "\t# match the features\n",
        "\tmethod = cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING\n",
        "\tmatcher = cv2.DescriptorMatcher_create(method)\n",
        "\tmatches = matcher.match(descsA, descsB, None)\n",
        " \n",
        " # sort the matches by their distance (the smaller the distance,\n",
        "\t# the \"more similar\" the features are)\n",
        "\tmatches = sorted(matches, key=lambda x:x.distance)\n",
        "\t# keep only the top matches\n",
        "\tkeep = int(len(matches) * keepPercent)\n",
        "\tmatches = matches[:keep]\n",
        "\t# check to see if we should visualize the matched keypoints\n",
        "\tif debug:\n",
        "\t\tmatchedVis = cv2.drawMatches(image, kpsA, template, kpsB,\n",
        "\t\t\tmatches, None)\n",
        "\t\tmatchedVis = imutils.resize(matchedVis, width=1000)\n",
        "\t\tcv2.imshow(\"Matched Keypoints\", matchedVis)\n",
        "\t\tcv2.waitKey(0)\n",
        "  \t# allocate memory for the keypoints (x, y)-coordinates from the\n",
        "\t# top matches -- we'll use these coordinates to compute our\n",
        "\t# homography matrix\n",
        "\tptsA = np.zeros((len(matches), 2), dtype=\"float\")\n",
        "\tptsB = np.zeros((len(matches), 2), dtype=\"float\")\n",
        "\t# loop over the top matches\n",
        "\tfor (i, m) in enumerate(matches):\n",
        "\t\t# indicate that the two keypoints in the respective images\n",
        "\t\t# map to each other\n",
        "\t\tptsA[i] = kpsA[m.queryIdx].pt\n",
        "\t\tptsB[i] = kpsB[m.trainIdx].pt\n",
        "\n",
        "    \t# compute the homography matrix between the two sets of matched\n",
        "\t# points\n",
        "\t(H, mask) = cv2.findHomography(ptsA, ptsB, method=cv2.RANSAC)\n",
        "\t# use the homography matrix to align the images\n",
        "\t(h, w) = template.shape[:2]\n",
        "\taligned = cv2.warpPerspective(image, H, (w, h))\n",
        "\t# return the aligned image\n",
        "\treturn aligned\n",
        "\n",
        "# import the necessary packages\n",
        "from pyimagesearch.alignment import align_images\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "\thelp=\"path to input image that we'll align to template\")\n",
        "ap.add_argument(\"-t\", \"--template\", required=True,\n",
        "\thelp=\"path to input template image\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# load the input image and template from disk\n",
        "print(\"[INFO] loading images...\")\n",
        "image = cv2.imread(args[\"image\"])\n",
        "template = cv2.imread(args[\"template\"])\n",
        "# align the images\n",
        "print(\"[INFO] aligning images...\")\n",
        "aligned = align_images(image, template, debug=True)\n",
        "\n",
        "# resize both the aligned and template images so we can easily\n",
        "# visualize them on our screen\n",
        "aligned = imutils.resize(aligned, width=700)\n",
        "template = imutils.resize(template, width=700)\n",
        "# our first output visualization of the image alignment will be a\n",
        "# side-by-side comparison of the output aligned image and the\n",
        "# template\n",
        "stacked = np.hstack([aligned, template])\n",
        "\n",
        "# our second image alignment visualization will be *overlaying* the\n",
        "# aligned image on the template, that way we can obtain an idea of\n",
        "# how good our image alignment is\n",
        "overlay = template.copy()\n",
        "output = aligned.copy()\n",
        "cv2.addWeighted(overlay, 0.5, output, 0.5, 0, output)\n",
        "# show the two output image alignment visualizations\n",
        "cv2.imshow(\"Image Alignment Stacked\", stacked)\n",
        "cv2.imshow(\"Image Alignment Overlay\", output)\n",
        "cv2.waitKey(0)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fae2f6223f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;31m# import the necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyimagesearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyimagesearch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d_exXeCijA6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}